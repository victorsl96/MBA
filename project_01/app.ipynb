{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Será que podemos comparar ideologias políticas a partir de tweets?\n",
    "\n",
    "ideologia: conjunto de convicções filosóficas, sociais, políticas etc. de um indivíduo ou grupo de indivíduos.\n",
    "\n",
    "Comparações:\n",
    "    - análise gráfica comparativa da frequência de palavras ou relacionadas a tópicos: \n",
    "        religião, educação, orientação sexual, identidade de gênero, economia, saúde, porte de armas, combate a pobreza\n",
    "            ex: Quantidade de tweets do Bolsonaro falando sobre drogas é muito maior que a do Lula, que mostra maior favor a política anti-drogas\n",
    "            (Buscar se a quantidade de palavras segue a mesma tendência)\n",
    "\n",
    "    - Comparação percentual entre palavras específicas em relação ao total\n",
    "\n",
    "    -! análise comparativa de tweets antes e depois de eleições\n",
    "\n",
    "Observações:\n",
    "    - contam retweets? -> Usar a função clean() para filtrar\n",
    "\n",
    "Proximos passos:\n",
    "Otimizar função de limpeza -> lemanização e estemização para remoção de radicais\n",
    "Comparar temas por datas específicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports section\n",
    "import json\n",
    "import spacy\n",
    "import re\n",
    "import pandas as pd\n",
    "import unicodedata as uni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    '''Open and read dataset files, saving contents as raw data.'''\n",
    "    with open('dataset/jairbolsonaro.json','r') as file_01:\n",
    "        data_01=json.load(file_01)\n",
    "    with open('dataset/LulaOficial.json','r') as file_02:\n",
    "        data_02=json.load(file_02)\n",
    "        \n",
    "    return data_01,data_02\n",
    "# get_data()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tweets():\n",
    "    '''Store tweets in lists without metadata'''\n",
    "    data_01,data_02 = get_data()\n",
    "    bolso_tweets = []\n",
    "    for el in data_01:\n",
    "        bolso_tweets.append(el['full_text'])\n",
    "\n",
    "    lula_tweets = []\n",
    "    for el in data_02:\n",
    "        lula_tweets.append(el['full_text'])\n",
    "        \n",
    "    return bolso_tweets, lula_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(bolso_tweets,lula_tweets,subject):\n",
    "    # Fix_01:Currently isn't useful because it is too restricted, doesn't account for synonyms and related words\n",
    "     \n",
    "    '''Compare tweets from both sides based on a subject and return a dataframe for comparison'''\n",
    "    # Look into Bolsonaro's tweets for the subject and store tweets in a list\n",
    "    b_selected = []\n",
    "    for tweet in bolso_tweets:\n",
    "        if subject.lower() in tweet.lower():\n",
    "            b_selected.append(tweet)\n",
    "\n",
    "    # Look into Lula's tweets for the subject and store tweets in a list\n",
    "    l_selected = []\n",
    "    for tweet in lula_tweets:\n",
    "        if subject.lower() in tweet.lower():\n",
    "            l_selected.append(tweet)\n",
    "\n",
    "    # Create two series based on the selected tweets lists\n",
    "    b = pd.Series(b_selected,name='Bolsonaro')\n",
    "    l = pd.Series(l_selected,name='Lula')\n",
    "\n",
    "    # Create dataframe concatenating the two series\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    df=pd.concat([b,l],axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmanize(tweet):\n",
    "    '''Lemmanization fuction. Remove radicals of words, grouping them together for analysis.'''\n",
    "    nlp=spacy.load('pt_core_news_sm')\n",
    "    result=nlp(tweet)\n",
    "    \n",
    "    # returns as a string for use in regular expression matching\n",
    "    return ' '.join([token.lemma_ for token in result])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(tweet):\n",
    "    '''Removes radicals of words to group them up, normalizes every word, removes links, hashtags, mentions, pontuation, emogis and removes stopwords from a single tweet and returns a list with the remaining words'''\n",
    "    \n",
    "    stopwords=('de', 'a', 'o', 'que', 'd','e', 'do', 'da', 'em', 'um', 'nao', 'para', 'e', 'com', 'uma', 'os', 'no', 'se', 'na', 'por', 'mais', 'as', 'dos', 'como', 'mas', 'foi', 'ao', 'ele', 'das', 'tem', 'seu', 'sua', 'ou', 'ser', 'quando', 'muito', 'ha', 'nos', 'ja', 'esta', 'eu', 'também', 'so', 'pelo', 'pela', 'ate', 'isso', 'ela', 'entre', 'era', 'depois', 'sem', 'mesmo', 'aos', 'ter', 'seus', 'quem', 'nas', 'me', 'esse', 'eles', 'estao', 'voce', 'tinha', 'foram', 'essa', 'num', 'nem', 'suas', 'meu', 'minha', 'numa', 'pelos', 'elas', 'havia', 'seja', 'qual', 'sera', 'tenho', 'lhe', 'deles', 'essas', 'esses', 'pelas', 'este', 'fosse', 'dele', 'tu', 'te', 'voces', 'vos', 'lhes', 'meus', 'minhas', 'teu', 'tua', 'teus', 'tuas', 'nosso', 'nossa', 'nossos', 'nossas', 'dela', 'delas', 'esta', 'estes', 'estas', 'aquele', 'aquela', 'aqueles', 'aquelas', 'isto', 'aquilo', 'estou', 'esta', 'estamos', 'estao', 'estive', 'esteve', 'estivemos', 'estiveram', 'estava', 'estavamos', 'estavam', 'estivera', 'estiveramos', 'esteja', 'estejamos', 'estejam', 'estivesse', 'estivessemos', 'estivessem', 'estiver', 'estivermos', 'estiverem', 'hei', 'ha', 'havemos', 'hao', 'houve', 'houvemos', 'houveram', 'houvera', 'houveramos', 'haja', 'hajamos', 'hajam', 'houvesse', 'houvessemos', 'houvessem', 'houver', 'houvermos', 'houverem', 'houverei', 'houvera', 'houveremos', 'houverao', 'houveria', 'houveriamos', 'houveriam', 'sou', 'somos', 'sao', 'era', 'eramos', 'eram', 'fui', 'foi', 'fomos', 'foram', 'fora', 'foramos', 'seja', 'sejamos', 'sejam', 'fosse', 'fossemos', 'fossem', 'for', 'formos', 'forem', 'serei', 'sera', 'seremos', 'serao', 'seria', 'seriamos', 'seriam', 'tenho', 'tem', 'temos', 'tem', 'tinha', 'tinhamos', 'tinham', 'tive', 'teve', 'tivemos', 'tiveram', 'tivera', 'tiveramos', 'tenha', 'tenhamos', 'tenham', 'tivesse', 'tivessemos', 'tivessem', 'tiver', 'tivermos', 'tiverem', 'terei', 'tera', 'teremos', 'terao', 'teria', 'teriamos', 'teriam')\n",
    "    \n",
    "    #lemmanize tweets in order to remove the radicals of words.\n",
    "    tweet=lemmanize(tweet)\n",
    "    \n",
    "    # normalize text and make all letters lowercase\n",
    "    tweet = uni.normalize('NFD', tweet).encode('ASCII', 'ignore').decode('utf-8').lower()\n",
    "    \n",
    "    # remove links\n",
    "    url_pattern=r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\\\".,<>?«»“”‘’]))'\n",
    "    \n",
    "    # remove hashtags\n",
    "    tags_pattern=r'\\#([a-zA-Z0-9_]{1,50})'\n",
    "    \n",
    "    # remove mentions\n",
    "    mentions_pattern=r'\\@([a-zA-Z0-9_]{1,50})'    \n",
    "    \n",
    "    # remove ponctuation and emogis \n",
    "    rest_pattern=r'[^\\w\\s]'\n",
    "    \n",
    "    # apply all patterns deleting all matched objects\n",
    "    patterns = [url_pattern,tags_pattern,mentions_pattern,rest_pattern]\n",
    "    for pattern in patterns:\n",
    "        tweet = re.sub(pattern,'',tweet)\n",
    "    \n",
    "    clean_tweet=[word for word in tweet.split() if word not in stopwords]\n",
    "    \n",
    "    return clean_tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(tweets,min=0):\n",
    "    '''Count the ocurrence of key words in the tweets'''\n",
    "    words={}\n",
    "    selected_words=[]\n",
    "    for tweet in tweets:\n",
    "        for word in clean(tweet):\n",
    "            if word in words.keys():\n",
    "                words[word] += 1\n",
    "            else:\n",
    "                words[word] = 1\n",
    "\n",
    "                \n",
    "    # Filter the words into a list based on a minimum number of ocurrences\n",
    "    for word, ocurrence in words.items():\n",
    "        if ocurrence > min:\n",
    "            selected_words.append(word)\n",
    "\n",
    "    # return selected_words\n",
    "    return sorted(words,key=words.get,reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bolso_tweets, lula_tweets = extract_tweets()\n",
    "# count_words(bolso_tweets)\n",
    "print(clean(bolso_tweets[1]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e163f6dc755e3f5ee4e51757169cbf4c5bf61c999acb41dad94c865ecb280cf3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
